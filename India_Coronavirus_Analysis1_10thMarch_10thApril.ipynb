{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/Users/f75k8bx/Documents/MyGitHub/NLP_using_News_API/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ind = df[df.country == 'India']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_Ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ind.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Date Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ind['publishedAt'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ind['published_at'] =  pd.to_datetime(df_Ind['publishedAt'], format='%Y-%m-%dT%H:%M:%SZ').dt.date\n",
    "df_Ind['published_at'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are - on an average - 1.5 to 2 sentences apart from the first line available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_tokenizer(text):\n",
    "    try:\n",
    "        if text:\n",
    "            sequences = sent_tokenize(text)\n",
    "            seq_tokens = [word_tokenize(seq) for seq in sequences]\n",
    "            no_punct_seq_tokens = []\n",
    "            for seq_token in seq_tokens:\n",
    "                no_punct_seq_tokens.append([token for token in seq_token if token not in string.punctuation and token not in STOPWORDS])\n",
    "            return no_punct_seq_tokens\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "path = os.getcwd()\n",
    "path_to_stnfrd_core_nlp = path + '/stanford-postagger/'\n",
    "\n",
    "jar = path_to_stnfrd_core_nlp + 'stanford-postagger.jar'\n",
    "model = path_to_stnfrd_core_nlp + 'models/english-bidirectional-distsim.tagger'\n",
    "\n",
    "st = StanfordPOSTagger(model, jar, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "dict_pos_map = {\n",
    "    # Look for NN in the POS tag because all nouns begin with NN\n",
    "    'NN': NOUN,\n",
    "    # Look for VB in the POS tag because all nouns begin with VB\n",
    "    'VB':VERB,\n",
    "    # Look for JJ in the POS tag because all nouns begin with JJ\n",
    "    'JJ' : ADJ,\n",
    "    # Look for RB in the POS tag because all nouns begin with RB\n",
    "    'RB':ADV  \n",
    "}\n",
    "\n",
    "def get_lemma(no_punct_seq_tokens):\n",
    "    try:\n",
    "        if no_punct_seq_tokens:\n",
    "            normalized_sequence = []\n",
    "            for each_seq in st.tag_sents(sentences=no_punct_seq_tokens):\n",
    "                normalized_tokens = []\n",
    "                for tuples in each_seq:\n",
    "                    temp = tuples[0]\n",
    "                    if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "                        continue\n",
    "                    if tuples[1][:2] in dict_pos_map.keys():\n",
    "                        temp = lm.lemmatize(tuples[0].lower(), \n",
    "                                            pos=dict_pos_map[tuples[1][:2]])\n",
    "                    normalized_tokens.append(temp)\n",
    "                normalized_sequence.append(normalized_tokens)\n",
    "            return normalized_sequence\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        print(no_punct_seq_tokens)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df, col_list):\n",
    "    for col in col_list:\n",
    "        print('Being: ' + col)\n",
    "        col_tokenized = col + '_tokenized'\n",
    "        col_normalised = col + '_normalized'\n",
    "        # tokenizer removes punctuations and stopwords\n",
    "        df[col_tokenized] = df[col].apply(lambda x : text_tokenizer(x))\n",
    "        df[col_normalised] = df[col_tokenized].apply(lambda x: get_lemma(x))\n",
    "        print('End: ' + col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"Start: \" + str(datetime.now()))\n",
    "\n",
    "pre_process(df_Ind, ['content', 'title', 'description'])\n",
    "\n",
    "print(\"End: \" + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(token_sequences):\n",
    "    wordcloud = WordCloud(width = 800, height = 800, max_words = 20, normalize_plurals = True,\n",
    "                background_color ='white').generate(token_sequences)\n",
    "\n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - Save the DF to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ind.to_pickle('pre_processed_df_ind.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_pickle('pre_processed_df_ind.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_title = [k for i in list(df_Ind['title_normalized']) for j in i for k in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(width = 1200, height = 800, max_words = 200, normalize_plurals = True,\n",
    "            background_color ='white', relative_scaling = 0.5, collocations = False, include_numbers = True,\n",
    "            stopwords = stopwords).generate(' '.join(list_title))\n",
    "\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud.words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runwordcloud(image, desc):\n",
    "\n",
    "    # based on example\n",
    "    # https://github.com/keyonvafa/inaugural-wordclouds/blob/master/create_wordclouds.py\n",
    "    \n",
    "    # get data and clean\n",
    "    print('Getting data...')\n",
    "    df = pd.DataFrame(get_lines(), columns=['text_raw'])\n",
    "    \n",
    "    print('Cleaning data...')\n",
    "    df = cleandata(df)\n",
    "    print(df.head(25).to_string())\n",
    "\n",
    "    print('Number of words', df['text_clean'].apply(lambda x: len(x.split(' '))).sum())\n",
    "\n",
    "    # import image\n",
    "    image_mask = np.array(Image.open(\"images/\"+image+\".jpeg\"))\n",
    "    image_colors = ImageColorGenerator(image_mask)\n",
    "    # generate wordcloud\n",
    "    print('Generating word cloud....')\n",
    "    wc = WordCloud(background_color=\"black\", width=400, height=400, max_words=2000, #contour_width=1, contour_color='red', \n",
    "    mask=image_mask, random_state=1).generate(' '.join(df['text_clean']))\n",
    "    \n",
    "    print('Making plot')\n",
    "    plt.figure(figsize=(20,10))\n",
    "    ypos = 650\n",
    "    \n",
    "    plt.style.use('dark_background')\n",
    "    #plt.imshow(wc.recolor(color_func=grey_color_func))\n",
    "    \n",
    "    # use image colours with white background\n",
    "    plt.imshow(wc.recolor(color_func=image_colors))\n",
    "\n",
    "    plt.text(0, ypos, \"Moi namesake\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('output/wordcloud_'+image+'_'+desc+'.png', dpi=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
