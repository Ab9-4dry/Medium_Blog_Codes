{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrdloi0tf-UV"
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgKBIqAWNc8u",
    "outputId": "8bbf8647-c94c-462d-a712-5511e8253a04"
   },
   "outputs": [],
   "source": [
    "# # Upload Kaggle json\n",
    "\n",
    "# !pip install -q kaggle\n",
    "# !pip install -q kaggle-cli\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp \"/content/drive/My Drive/Kaggle/kaggle.json\" ~/.kaggle/ # Mount GDrive\n",
    "# !cat ~/.kaggle/kaggle.json \n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle competitions download -c fake-news -p dataset\n",
    "# !unzip /content/dataset/train.csv.zip\n",
    "# !unzip /content/dataset/test.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYBAcN3JOmkI",
    "outputId": "1a5ffc38-ecb4-4b81-e6f6-9be0d951c9b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nroy0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nroy0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nroy0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Using cached contractions-0.0.48-py2.py3-none-any.whl (6.4 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Using cached textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting pyahocorasick\n",
      "  Using cached pyahocorasick-1.4.2.tar.gz (321 kB)\n",
      "Requirement already satisfied: anyascii in c:\\users\\nroy0\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n",
      "Building wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py): started\n",
      "  Building wheel for pyahocorasick (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pyahocorasick\n",
      "Failed to build pyahocorasick\n",
      "Installing collected packages: pyahocorasick, textsearch, contractions\n",
      "    Running setup.py install for pyahocorasick: started\n",
      "    Running setup.py install for pyahocorasick: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\nroy0\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\nroy0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-hgawtbmu\\\\pyahocorasick\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\nroy0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-hgawtbmu\\\\pyahocorasick\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\nroy0\\AppData\\Local\\Temp\\pip-wheel-65rqdycn'\n",
      "       cwd: C:\\Users\\nroy0\\AppData\\Local\\Temp\\pip-install-hgawtbmu\\pyahocorasick\\\n",
      "  Complete output (5 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'ahocorasick' extension\n",
      "  error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyahocorasick\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\nroy0\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\nroy0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-hgawtbmu\\\\pyahocorasick\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\nroy0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-hgawtbmu\\\\pyahocorasick\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\nroy0\\AppData\\Local\\Temp\\pip-record-dmuhxs72\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\nroy0\\anaconda3\\Include\\pyahocorasick'\n",
      "         cwd: C:\\Users\\nroy0\\AppData\\Local\\Temp\\pip-install-hgawtbmu\\pyahocorasick\\\n",
      "    Complete output (5 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_ext\n",
      "    building 'ahocorasick' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\nroy0\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\nroy0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-hgawtbmu\\\\pyahocorasick\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\nroy0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-hgawtbmu\\\\pyahocorasick\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\nroy0\\AppData\\Local\\Temp\\pip-record-dmuhxs72\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\nroy0\\anaconda3\\Include\\pyahocorasick' Check the logs for full command output.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d6a974f8eb73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install contractions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import contractions\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Thtim9SaOmpW"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('/content/train.csv', header=0)\n",
    "# test_df = pd.read_csv('/content/test.csv', header=0)\n",
    "\n",
    "# train_df = pd.read_csv('/content/train.csv', header=0)\n",
    "# test_df = pd.read_csv('/content/test.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtlE2tgeK9TZ",
    "outputId": "f4550b93-f77a-46d5-a52c-c4b6e1174cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.fillna(' ')\n",
    "test_df = test_df.fillna(' ')\n",
    "\n",
    "train_df['text'] = train_df['text'].str.strip()\n",
    "test_df['text'] = test_df['text'].str.strip()\n",
    "\n",
    "train_df['raw_text_length'] = train_df['text'].apply(lambda x: len(x))\n",
    "print(len(train_df[train_df['raw_text_length']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1frVhmo8L0Lk",
    "outputId": "200ee4a9-5968-49bd-e590-0e570247a5f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20684, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[train_df['raw_text_length'] > 0]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cZ0vgCArkSMr"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(x):\n",
    "  try:\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\d\\s\\']+', '', x)\n",
    "    word_list = []\n",
    "    for each_word in cleaned_text.split(' '):\n",
    "        word_list.append(contractions.fix(each_word).lower())\n",
    "  except:\n",
    "    print(x)\n",
    "  return \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UzyElDkakeoE"
   },
   "outputs": [],
   "source": [
    "text_cols = ['text', 'title', 'author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7tWp8Awl6LZ",
    "outputId": "43395733-2752-4fbc-cd3c-c3708ea7e54e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: text\n",
      "Processing train data... \n",
      "Processing test data... \n",
      "Processing column: title\n",
      "Processing train data... \n",
      "Processing test data... \n",
      "Processing column: author\n",
      "Processing train data... \n",
      "Processing test data... \n",
      "CPU times: user 1min, sys: 222 ms, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for col in text_cols:\n",
    "  print(\"Processing column: {}\".format(col))\n",
    "  print(\"Processing train data... \")\n",
    "  train_df[col] = train_df[col].apply(lambda x: preprocess_text(x))\n",
    "  print(\"Processing test data... \")\n",
    "  test_df[col] = test_df[col].apply(lambda x: preprocess_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MFupIrYikkB",
    "outputId": "cb610713-c10b-4ea3-de8f-b19acad66e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: text\n",
      "Processing train data... \n",
      "Processing test data... \n",
      "Processing column: title\n",
      "Processing train data... \n",
      "Processing test data... \n",
      "Processing column: author\n",
      "Processing train data... \n",
      "Processing test data... \n",
      "CPU times: user 1min 2s, sys: 644 ms, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for col in text_cols:\n",
    "  print(\"Processing column: {}\".format(col))\n",
    "  print(\"Processing train data... \")\n",
    "  train_df[col] = train_df[col].apply(word_tokenize)\n",
    "  print(\"Processing test data... \")\n",
    "  test_df[col] = test_df[col].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpREdIrlikpT",
    "outputId": "26a3d284-6e64-4648-f61d-38ae5998c27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: text\n",
      "Processing train data... \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for col in text_cols:\n",
    "  print(\"Processing column: {}\".format(col))\n",
    "  print(\"Processing train data... \")\n",
    "  train_df[col] = train_df[col].apply(lambda x: [each_word for each_word in x if each_word not in stopwords])\n",
    "  print(\"Processing test data... \")\n",
    "  test_df[col] = test_df[col].apply(lambda x: [each_word for each_word in x if each_word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3smbWP0btIf"
   },
   "outputs": [],
   "source": [
    "print(\"Processing train data... \")\n",
    "train_df['all_info'] = train_df['text'] + train_df['title'] + train_df['author'] \n",
    "train_df['all_info'] = train_df['all_info'].apply(lambda x: \" \".join(x))\n",
    "print(\"Processing test data... \")\n",
    "test_df['all_info'] = test_df['text'] + test_df['title'] + test_df['author'] \n",
    "test_df['all_info'] = test_df['all_info'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LtCO-hxqAaz"
   },
   "outputs": [],
   "source": [
    "all_text_train = train_df[\"all_info\"].astype(str).tolist()\n",
    "all_text_test = test_df[\"all_info\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhIV-uCvCeBJ"
   },
   "outputs": [],
   "source": [
    "target = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iw7qydwRiSML"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token = \"<OOV>\", num_words=6000)\n",
    "tokenizer.fit_on_texts(all_text_train)\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPrbUDumiSS6"
   },
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(all_text_train)\n",
    "padded_train = pad_sequences(sequences_train, padding = 'post', maxlen=6000)\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(all_text_test)\n",
    "padded_test = pad_sequences(sequences_test, padding = 'post', maxlen=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS6RnrrkiSXZ"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model=Sequential()\n",
    "  model.add(Embedding(6000, 300 ,input_length=6000))\n",
    "  model.add(Dropout(0.3))\n",
    "  model.add(LSTM(200))\n",
    "  model.add(Dropout(0.3))\n",
    "  model.add(Dense(64,activation='relu'))\n",
    "  model.add(Dropout(0.3))\n",
    "  model.add(Dense(1,activation='sigmoid'))\n",
    "  return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOMe15iMiSVk"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_train, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_rBhLiwiSRy"
   },
   "outputs": [],
   "source": [
    "callbacks=[\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, \n",
    "                                  verbose=1, mode=\"min\", restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint(filepath=\"best_model.hdf5\", verbose=1, save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31ofUO6biSO4"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AggtZp00soXb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=20,\n",
    "                    batch_size=64, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0R_WwTSiSHX"
   },
   "outputs": [],
   "source": [
    "metric_toplot = \"loss\"\n",
    "plt.plot(history.epoch, history.history[metric_toplot], \".:\", label=\"loss\")\n",
    "plt.plot(history.epoch, history.history[\"val_\"+metric_toplot], \".:\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IpYPyH-soI6"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSDcImK4bw0v"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G93AtXOfSGFd"
   },
   "outputs": [],
   "source": [
    "y_test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_2Us8-DsoDt"
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usiMK0OJsn-i"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BTKxitGsn7Z"
   },
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPe2fpCLLxaB"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cH_EGgmPbYbh"
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict_classes(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Y5KG-GzadOb"
   },
   "outputs": [],
   "source": [
    "submit_lstm_predictions_df = pd.DataFrame()\n",
    "submit_lstm_predictions_df['id'] = test_df['id']\n",
    "submit_lstm_predictions_df['label'] = y_pred_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHpAxyxWaftg"
   },
   "outputs": [],
   "source": [
    "submit_lstm_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9kMPl23JG-I"
   },
   "outputs": [],
   "source": [
    "submit_lstm_predictions_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mr7Q4WgHsnxi"
   },
   "outputs": [],
   "source": [
    "sns.countplot(submit_lstm_predictions_df['label'])\n",
    "plt.title(\"Count of Genuine and Fake News of the Test Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBBySUC1bEJ1"
   },
   "outputs": [],
   "source": [
    "submit_predictions_df.to_csv('submit.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fake News using LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
