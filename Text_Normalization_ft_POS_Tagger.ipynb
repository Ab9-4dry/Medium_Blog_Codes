{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP<3 I : Text Normalization using POS Taggers\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hermione_said = '''Books! And cleverness! There are more important things - friendship and bravery and - oh Harry - be careful!'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.  Tokenization Processes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenize\n",
    "\n",
    "collection of sequential groups of words. Each sequence ~ each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Books!',\n",
       " 'And cleverness!',\n",
       " 'There are more important things - friendship and bravery and - oh Harry - be careful!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "sequences = sent_tokenize(hermione_said)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenize\n",
    "\n",
    "collection of words - or more appropriately tokens - that form a sequence (or sentence)\n",
    "While sentences, comprised of lexical words should have a meaning, a sequence of tokens might not bear any significant meaning at first glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Books', '!'],\n",
       " ['And', 'cleverness', '!'],\n",
       " ['There',\n",
       "  'are',\n",
       "  'more',\n",
       "  'important',\n",
       "  'things',\n",
       "  '-',\n",
       "  'friendship',\n",
       "  'and',\n",
       "  'bravery',\n",
       "  'and',\n",
       "  '-',\n",
       "  'oh',\n",
       "  'Harry',\n",
       "  '-',\n",
       "  'be',\n",
       "  'careful',\n",
       "  '!']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tokens = [word_tokenize(seq) for seq in sequences]\n",
    "seq_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens itself does not bear any meaning. For example the tokens containing only puctuations does not bear any meaning. In fact, in isolated states, words like \"books\", \"and\", \"friendship\" has dictionary meaning but huhman communication is always contextual and context is difficult to decipher from single words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "no_punct_seq_tokens = []\n",
    "\n",
    "for seq_token in seq_tokens:\n",
    "    no_punct_seq_tokens.append([token for token in seq_token if token not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Books'],\n",
       " ['And', 'cleverness'],\n",
       " ['There',\n",
       "  'are',\n",
       "  'more',\n",
       "  'important',\n",
       "  'things',\n",
       "  'friendship',\n",
       "  'and',\n",
       "  'bravery',\n",
       "  'and',\n",
       "  'oh',\n",
       "  'Harry',\n",
       "  'be',\n",
       "  'careful']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punct_seq_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Normalization Techniques - Stemming and Lemmatization\n",
    "---\n",
    "\n",
    "Resolving ambiguity by reducing tokens to inflectional forms or other derivational forms to a common base form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is used to reduce different grammatical forms or word forms of a word like its noun, adjective, verb, adverb etc. to its root form. Computationally, it is a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. It is important for information retrieval systems.\n",
    "\n",
    "Ref: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Porter Stemmer implementation in nltk\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'and',\n",
       " 'clever',\n",
       " 'there',\n",
       " 'are',\n",
       " 'more',\n",
       " 'import',\n",
       " 'thing',\n",
       " 'friendship',\n",
       " 'and',\n",
       " 'braveri',\n",
       " 'and',\n",
       " 'oh',\n",
       " 'harri',\n",
       " 'be',\n",
       " 'care']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_tokens = [stemmer.stem(token) for seq in no_punct_seq_tokens for token in seq]\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Stemming is rule based and thus we can see the changes like \"braveri\" and \"harri\" which does not make sense. Also, notice all the words are already transformed into lower case. This poses a challenge for proper noun detection because the only significant physical notation - the first letter in upper case - will not more be in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "Ref: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Books',\n",
       " 'And',\n",
       " 'cleverness',\n",
       " 'There',\n",
       " 'are',\n",
       " 'more',\n",
       " 'important',\n",
       " 'thing',\n",
       " 'friendship',\n",
       " 'and',\n",
       " 'bravery',\n",
       " 'and',\n",
       " 'oh',\n",
       " 'Harry',\n",
       " 'be',\n",
       " 'careful']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens = [lm.lemmatize(token) for seq in no_punct_seq_tokens for token in seq]\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite naive. The only thing that has changed is \"thing\" to \"things\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now, we will make use of POS argument and try to lemmatize again and test a few variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('run', 'running')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize(\"running\", pos=\"v\"), lm.lemmatize(\"running\", pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Harry', 'Harry', 'harry')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize(\"Harry\", pos=\"n\"), lm.lemmatize(\"Harry\", pos=\"v\"), lm.lemmatize(\"harry\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Books', 'Books')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize(\"Books\", pos=\"n\"), lm.lemmatize(\"Books\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('book', 'book')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize(\"books\", pos=\"v\"), lm.lemmatize(\"books\", pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('more', 'good', 'best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize(\"more\", pos=\"a\"), lm.lemmatize(\"better\", pos=\"a\"), lm.lemmatize(\"best\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. POS TAGGER\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Penn Treebank tag | Explanation\n",
    "---|---\n",
    "CC  | Coordinating conjunction\n",
    "CD |  Cardinal number\n",
    "DT |  Determiner\n",
    "EX |  Existential there\n",
    "FW |  Foreign word\n",
    "IN |  Preposition or subordinating conjunction\n",
    "JJ |  Adjective\n",
    "JJR | Adjective, comparative\n",
    "JJS | Adjective, superlative\n",
    "LS  | List item marker\n",
    "MD  | Modal\n",
    "NN  | Noun, singular or mass\n",
    "NNS | Noun, plural\n",
    "NNP | Proper noun, singular\n",
    "NNPS | Proper noun, plural\n",
    "PDT | Predeterminer\n",
    "POS | Possessive ending\n",
    "PRP | Personal pronoun\n",
    "PRP\\$|Possessive pronoun\n",
    "RB |  Adverb\n",
    "RBR | Adverb, comparative\n",
    "RBS | Adverb, superlative\n",
    "RP  |Particle\n",
    "SYM |Symbol\n",
    "TO | to\n",
    "UH  | Interjection\n",
    "VB  | Verb, base form\n",
    "VBD | Verb, past tense\n",
    "VBG | Verb, gerund or present participle\n",
    "VBN | Verb, past participle\n",
    "VBP | Verb, non 3rd person singular present\n",
    "VBZ | Verb, 3rd person singular present\n",
    "WDT | Wh determiner\n",
    "WP |  Wh pronoun\n",
    "WRB | Wh adverb\n",
    "WP\\$ | Possessive wh pronoun\n",
    "\n",
    "To learn more about the tags, visit-\n",
    "\n",
    "* https://www.eecis.udel.edu/~vijay/cis889/ie/pos-set.pdf\n",
    "* http://erwinkomen.ruhosting.nl/eng/2014_Longdale-Labels.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "path = os.getcwd()\n",
    "path_to_stnfrd_core_nlp = path + '/stanford-postagger/'\n",
    "\n",
    "jar = path_to_stnfrd_core_nlp + 'stanford-postagger.jar'\n",
    "model = path_to_stnfrd_core_nlp + 'models/english-bidirectional-distsim.tagger'\n",
    "\n",
    "st = StanfordPOSTagger(model, jar, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tags_lemmatized_tokens = st.tag(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Books', 'NNS'),\n",
       " ('And', 'CC'),\n",
       " ('cleverness', 'NN'),\n",
       " ('There', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('more', 'RBR'),\n",
       " ('important', 'JJ'),\n",
       " ('thing', 'NN'),\n",
       " ('friendship', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('bravery', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('oh', 'UH'),\n",
       " ('Harry', 'NNP'),\n",
       " ('be', 'VB'),\n",
       " ('careful', 'JJ')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tags_lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tags_stemmed_tokens = st.tag(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('clever', 'JJ'),\n",
       " ('there', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('more', 'JJR'),\n",
       " ('import', 'NN'),\n",
       " ('thing', 'NN'),\n",
       " ('friendship', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('braveri', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('oh', 'UH'),\n",
       " ('harri', 'NNS'),\n",
       " ('be', 'VB'),\n",
       " ('care', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tags_stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to choose - Stemming or Lemma or both?\n",
    "\n",
    "This little snippet is gives a taster in the larger challenges that NLP practitioners face when dealing with tokens. \n",
    "\n",
    "For example, the first token - **book** - the stemmed token's tag is NN (Noun) and that of the lemmatized one is NNS (Plural Noun). Which seems better? To answer this question, we need to take a step back and identify answers to questions like:\n",
    "\n",
    "* What is the problem statement?\n",
    "* What features are important to address the problem statement?\n",
    "* Is this featue an overhead for computation?\n",
    "\n",
    "Second, **Harry** - which is firstly wrongly stemmed to *harri* and therefore the POS tagger fails to identify it correctly as a Proper Noun while the lemmatized token correctly classified *Harry*. Besides, like, **braveri** - even though these words are not anywhere in the english lexical dictionary, should have been classified as a FW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING POS TAGGED TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Books', 'NNS')],\n",
       " [('And', 'CC'), ('cleverness', 'NN')],\n",
       " [('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('more', 'RBR'),\n",
       "  ('important', 'JJ'),\n",
       "  ('things', 'NNS'),\n",
       "  ('friendship', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('bravery', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('oh', 'UH'),\n",
       "  ('Harry', 'NNP'),\n",
       "  ('be', 'VB'),\n",
       "  ('careful', 'JJ')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token sequences tagging\n",
    "st.tag_sents(sentences=no_punct_seq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a gold set or the expected results to measure the performance of the POS tagger model. In this new variable, I have only corrected theh symbols. I am happy with all other tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = [[('Books', 'NNS')],\n",
    " [('And', 'CC'), ('cleverness', 'NN')],\n",
    " [('There', 'EX'),\n",
    "  ('are', 'VBP'),\n",
    "  ('more', 'RBR'),\n",
    "  ('important', 'JJ'),\n",
    "  ('things', 'NNS'),\n",
    "  ('friendship', 'NN'),\n",
    "  ('and', 'CC'),\n",
    "  ('bravery', 'NN'),\n",
    "  ('and', 'CC'),\n",
    "  ('oh', 'UH'),\n",
    "  ('Harry', 'NNP'),\n",
    "  ('be', 'VB'),\n",
    "  ('careful', 'JJ')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Books', 'NNS')],\n",
       " [('And', 'CC'), ('cleverness', 'NN')],\n",
       " [('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('more', 'RBR'),\n",
       "  ('important', 'JJ'),\n",
       "  ('things', 'NNS'),\n",
       "  ('friendship', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('bravery', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('oh', 'UH'),\n",
       "  ('Harry', 'NNP'),\n",
       "  ('be', 'VB'),\n",
       "  ('careful', 'JJ')]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.evaluate(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLVING THE PROBLEMS DISCUSSED\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Observations:\n",
    "\n",
    "* The Lemma worked fine but the stemming results were sometimes in correct\n",
    "* The POS tags worked well when the cases of the words were preserved. Otherwise, the outcomes were incorrect\n",
    "* We also have a \"pos\" argument in lemmatization which could be used to get better results\n",
    "\n",
    "### OBJECTIVE: To normalize correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** To access the pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books NNS\n",
      "And CC\n",
      "cleverness NN\n",
      "There EX\n",
      "are VBP\n",
      "more RBR\n",
      "important JJ\n",
      "things NNS\n",
      "friendship NN\n",
      "and CC\n",
      "bravery NN\n",
      "and CC\n",
      "oh UH\n",
      "Harry NNP\n",
      "be VB\n",
      "careful JJ\n"
     ]
    }
   ],
   "source": [
    "for each_seq in st.tag_sents(sentences=no_punct_seq_tokens):\n",
    "    for tuples in each_seq:\n",
    "        print(tuples[0], tuples[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** To create a mapper for the arguments to wordnet according to the treebank POS tag codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pos_map = {\n",
    "    # Look for NN in the POS tag because all nouns begin with NN\n",
    "    'NN': NOUN,\n",
    "    # Look for VB in the POS tag because all nouns begin with VB\n",
    "    'VB':VERB,\n",
    "    # Look for JJ in the POS tag because all nouns begin with JJ\n",
    "    'JJ' : ADJ,\n",
    "    # Look for RB in the POS tag because all nouns begin with RB\n",
    "    'RB':ADV  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** To get the lemmas accoridngly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "cleverness\n",
      "be\n",
      "more\n",
      "important\n",
      "thing\n",
      "friendship\n",
      "bravery\n",
      "be\n",
      "careful\n"
     ]
    }
   ],
   "source": [
    "for each_seq in st.tag_sents(sentences=no_punct_seq_tokens):\n",
    "    for tuples in each_seq:\n",
    "        if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "            continue\n",
    "        if tuples[1][:2] in dict_pos_map.keys():\n",
    "            print(lm.lemmatize(tuples[0].lower(), pos=dict_pos_map[tuples[1][:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Adding stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['book'],\n",
       " ['and', 'clever'],\n",
       " ['there',\n",
       "  'be',\n",
       "  'more',\n",
       "  'import',\n",
       "  'thing',\n",
       "  'friendship',\n",
       "  'and',\n",
       "  'braveri',\n",
       "  'and',\n",
       "  'oh',\n",
       "  'be',\n",
       "  'care']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_sequence = []\n",
    "for each_seq in st.tag_sents(sentences=no_punct_seq_tokens):\n",
    "    normalized_tokens = []\n",
    "    for tuples in each_seq:\n",
    "        temp = tuples[0]\n",
    "        if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "            continue\n",
    "        if tuples[1][:2] in dict_pos_map.keys():\n",
    "            temp = lm.lemmatize(tuples[0].lower(), \n",
    "                                pos=dict_pos_map[tuples[1][:2]])\n",
    "        temp = stemmer.stem(temp)\n",
    "        normalized_tokens.append(temp)\n",
    "    normalized_sequence.append(normalized_tokens)\n",
    "normalized_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's better and I know the Proper Noun - Harry - is retained as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks for visiting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
